{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import zipfile\n",
    "import random\n",
    "# from kafka import KafkaProducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serializer(message):\n",
    "    return json.dumps(message).encode('utf-8')\n",
    "# Thêm địa chỉ của tất cả các broker vào danh sách dưới đây\n",
    "bootstrap_servers = ['localhost:9092', 'localhost:9093', 'localhost:9094']\n",
    "\n",
    "# Key serializer\n",
    "def key_serializer(key):\n",
    "    return str(key).encode('utf-8')\n",
    "\n",
    "\n",
    "\n",
    "# # Kafka Producer with custom partitioner\n",
    "# producer = KafkaProducer(\n",
    "#     bootstrap_servers=bootstrap_servers,\n",
    "#     value_serializer=serializer,\n",
    "#     key_serializer=key_serializer,\n",
    "# )\n",
    "\n",
    "\n",
    "# producer.send('user', value=dummy_message, key = random.choice(range(0,5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PATH constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"craw\"\n",
    "HTML_PATH = BASE_PATH + \"/html\"\n",
    "USER_PATH = BASE_PATH + \"/users\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get Clubs IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1"
     ]
    }
   ],
   "source": [
    "def get_number(string):\n",
    "    return int(string.strip().replace(\",\", \"\"))\n",
    "\n",
    "\n",
    "clubs_id = set()\n",
    "possibles_users = 0\n",
    "page = 1\n",
    "\n",
    "while True:\n",
    "    print(f\"\\r{page}\", end=\"\")\n",
    "\n",
    "    time.sleep(3)  # Wait 3 seconds per page\n",
    "    data = requests.get(f\"https://myanimelist.net/clubs.php?p={page}\")\n",
    "    soup = BeautifulSoup(data.text, \"html.parser\")\n",
    "    rows = soup.find_all(\"tr\", {\"class\": \"table-data\"})\n",
    "    for row in rows:\n",
    "        members = get_number(row.find(\"td\", {\"class\": \"ac\"}).text)\n",
    "        club_id = get_number(\n",
    "            row.find(\"a\", {\"class\": \"fw-b\"}).get(\"href\").split(\"=\")[-1]\n",
    "        )\n",
    "        if (\n",
    "            club_id not in clubs_id and members > 30\n",
    "        ):  # Only save groups with more than 30 members\n",
    "            possibles_users += members\n",
    "            clubs_id.add(club_id)\n",
    "\n",
    "    page += 1\n",
    "    # if possibles_users > 1000000:  # Threshold to stop\n",
    "    if possibles_users > 10:  # Threshold to stop\n",
    "        break\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(BASE_PATH, exist_ok=True)\n",
    "\n",
    "# Save the club IDs to the file\n",
    "with open(os.path.join(BASE_PATH, \"clubs.txt\"), \"w\") as file:\n",
    "    for club in clubs_id:\n",
    "        file.write(f\"{club}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get usernames in every clubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{BASE_PATH}/users_list.txt\"):\n",
    "    with open(f\"{BASE_PATH}/users_list.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
    "        pass\n",
    "    \n",
    "if not os.path.exists(f\"{BASE_PATH}/_revised_clubs.txt\"):\n",
    "    with open(f\"{BASE_PATH}/_revised_clubs.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 42)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"{BASE_PATH}/clubs.txt\") as file:\n",
    "    clubs_id = [x.strip() for x in file.readlines()]\n",
    "\n",
    "with open(f\"{BASE_PATH}/users_list.txt\", encoding=\"UTF-8\") as file:\n",
    "    users = set([x.strip() for x in file.readlines()])\n",
    "\n",
    "with open(f\"{BASE_PATH}/_revised_clubs.txt\", encoding=\"UTF-8\") as file:\n",
    "    revised_clubs = set([int(x.strip()) for x in file.readlines()])\n",
    "\n",
    "len(users), len(revised_clubs), len(clubs_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 --> 02"
     ]
    }
   ],
   "source": [
    "#%%notify -m \"get username finish\"\n",
    "\n",
    "for i, club_id in enumerate(clubs_id):\n",
    "    if club_id in revised_clubs:\n",
    "        continue\n",
    "\n",
    "    # page = 1\n",
    "    # while True:\n",
    "    print(f\"\\r{i+1}/{len(clubs_id)} --> {str(page).zfill(2)}\", end=\"\")\n",
    "    # link = f\"https://api.jikan.moe/v3/club/{club_id}/members/{page}\"\n",
    "    link = f\"https://api.jikan.moe/v4/clubs/{club_id}/members\"\n",
    "\n",
    "    try:\n",
    "        time.sleep(4.2)\n",
    "        data = requests.get(link)\n",
    "    except KeyboardInterrupt:\n",
    "        raise KeyboardInterrupt()\n",
    "    except:  # Other exception wait 2 min and try again\n",
    "        time.sleep(120)\n",
    "        continue\n",
    "\n",
    "    if data.status_code != 200:\n",
    "        break\n",
    "\n",
    "    with open(f\"{BASE_PATH}/users_list.txt\", \"a\", encoding=\"UTF-8\") as file:\n",
    "        for user in map(lambda x: x[\"username\"], json.loads(data.text)[\"data\"]):\n",
    "            if user not in users and user != \"\":\n",
    "                file.write(f\"{user}\\n\")\n",
    "                users.add(user)\n",
    "        # page += 1\n",
    "\n",
    "    revised_clubs.add(club_id)\n",
    "    with open(f\"{BASE_PATH}/_revised_clubs.txt\", \"a\", encoding=\"UTF-8\") as file:\n",
    "        file.write(f\"{club_id}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{BASE_PATH}/users_list.txt\", encoding=\"UTF-8\") as file:\n",
    "    users = list(set([x.strip() for x in file.readlines()]))[1:]\n",
    "    random.shuffle(users)\n",
    "\n",
    "with open(f\"{BASE_PATH}/users.csv\", \"w\", encoding=\"UTF-8\") as file:\n",
    "    file.write(\"user_id,username\\n\")\n",
    "    for i, user in enumerate(users):\n",
    "        file.write(f\"{i},{user}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get animelist per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1270, -1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"{BASE_PATH}/users.csv\", \"r\", encoding=\"UTF-8\") as file:\n",
    "    file.readline()\n",
    "    users = [x.strip().split(\",\") for x in file.readlines()]\n",
    "    users = [(int(x[0]), x[1]) for x in users]\n",
    "\n",
    "last_revised_users = -1\n",
    "if os.path.exists(f\"{BASE_PATH}/_last_revised_users.txt\"):\n",
    "    with open(f\"{BASE_PATH}/_last_revised_users.txt\", \"r\", encoding=\"UTF-8\") as file:\n",
    "        last_revised_users = int(file.readline())\n",
    "\n",
    "len(users), last_revised_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-17 08:04:37 --> 8/1270Acosmist\n",
      "2023-12-17 08:07:58 --> 29/1270Amelix\n",
      "2023-12-17 08:11:21 --> 46/1270-Auria-\n",
      "2023-12-17 08:14:42 --> 63/1270Infamous_Empire\n",
      "2023-12-17 08:21:00 --> 97/1270-cutthroat954\n",
      "2023-12-17 08:24:45 --> 118/1270Beatrize\n",
      "2023-12-17 08:31:19 --> 147/1270AdmiralScarlet-\n",
      "2023-12-17 08:41:01 --> 194/1270123114334d\n",
      "2023-12-17 08:43:58 --> 210/1270Glazuelo\n",
      "2023-12-17 08:44:54 --> 214/1270-Roxana\n",
      "2023-12-17 08:45:21 --> 218/1270-Azazel-san\n",
      "2023-12-17 08:54:56 --> 273/1270Bloodylily703\n",
      "2023-12-17 08:56:19 --> 283/1270-Bass\n",
      "2023-12-17 09:01:10 --> 306/1270Chellle\n",
      "2023-12-17 09:09:01 --> 338/1270Anhicek\n",
      "2023-12-17 09:12:40 --> 356/1270-aka-\n",
      "2023-12-17 09:13:11 --> 361/1270000mohit\n",
      "2023-12-17 09:13:27 --> 363/1270-lolitsun\n",
      "2023-12-17 09:15:10 --> 376/1270-Luk134-\n",
      "2023-12-17 09:19:00 --> 390/1270Aqsul\n",
      "2023-12-17 09:20:50 --> 398/1270afganiasakrisna\n",
      "2023-12-17 09:21:34 --> 402/1270AfterHours\n",
      "2023-12-17 09:23:19 --> 409/1270Sereshay\n",
      "2023-12-17 09:24:57 --> 416/1270AnimeFanUnknown\n",
      "2023-12-17 09:25:18 --> 420/1270-chronos\n",
      "2023-12-17 09:40:12 --> 494/1270-aejay-\n",
      "2023-12-17 09:41:12 --> 498/1270Disapeared_Ghost\n",
      "2023-12-17 09:47:13 --> 525/1270-Alexandru\n",
      "2023-12-17 09:58:09 --> 581/1270-Magical_Star-\n",
      "2023-12-17 10:04:24 --> 596/1270-VictoR-\n",
      "2023-12-17 10:07:04 --> 609/1270ap3897\n",
      "2023-12-17 10:08:36 --> 617/1270saelogy\n",
      "2023-12-17 10:13:27 --> 639/1270novio\n",
      "2023-12-17 10:14:21 --> 644/1270-HippySnob-\n",
      "2023-12-17 10:16:31 --> 651/1270-Rekha-\n",
      "2023-12-17 10:19:26 --> 659/12707KpR3q2XeF9oWtPv\n",
      "2023-12-17 10:20:45 --> 668/1270-Spada-\n",
      "2023-12-17 10:22:40 --> 682/12704iwby\n",
      "2023-12-17 10:23:54 --> 690/1270-HAIME-\n",
      "2023-12-17 10:24:42 --> 696/1270-Karoshi-\n",
      "2023-12-17 10:26:26 --> 709/1270-0-0-0-0-0-\n",
      "2023-12-17 10:26:31 --> 710/1270-Airin\n",
      "2023-12-17 10:27:50 --> 718/1270Amalouu\n",
      "2023-12-17 10:36:40 --> 766/1270adi_r\n",
      "2023-12-17 10:36:50 --> 768/1270-Coraline-\n",
      "2023-12-17 10:40:24 --> 784/1270-Ryuei\n",
      "2023-12-17 10:54:28 --> 807/1270AniAniMangaXll\n",
      "2023-12-17 10:58:05 --> 824/1270-Exo-\n",
      "2023-12-17 10:59:17 --> 831/1270aizuki\n",
      "2023-12-17 11:01:47 --> 840/1270tsubasalover\n",
      "2023-12-17 11:09:06 --> 878/1270Sakuragi900\n",
      "2023-12-17 11:10:55 --> 889/1270Promise\n",
      "2023-12-17 11:11:43 --> 893/1270otakudashie\n",
      "2023-12-17 11:14:47 --> 907/1270Alaska115\n",
      "2023-12-17 11:16:45 --> 919/1270BroccoliBoii\n",
      "2023-12-17 11:17:11 --> 923/1270Ahxxn\n",
      "2023-12-17 11:26:53 --> 969/12709133\n",
      "2023-12-17 11:33:17 --> 1011/1270atychi\n",
      "2023-12-17 11:34:56 --> 1017/1270anklee\n",
      "2023-12-17 11:35:37 --> 1024/12701bn_Meltdown\n",
      "2023-12-17 11:42:43 --> 1047/1270AdmnUnpwnd\n",
      "2023-12-17 11:45:04 --> 1056/1270--Tapioca--\n",
      "2023-12-17 11:47:58 --> 1070/1270-Knightingale\n",
      "2023-12-17 11:49:44 --> 1082/1270CorayyAnime\n",
      "2023-12-17 11:50:00 --> 1085/127032452453452\n",
      "2023-12-17 11:51:51 --> 1097/1270aphros\n",
      "2023-12-17 11:55:10 --> 1109/1270Startlightanya\n",
      "2023-12-17 11:59:12 --> 1129/1270-ViviChan-\n",
      "2023-12-17 11:59:53 --> 1132/1270Adammegaprs\n",
      "2023-12-17 12:00:35 --> 1138/1270-Doctor-\n",
      "2023-12-17 12:03:19 --> 1150/1270Annie_Law\n",
      "2023-12-17 12:11:13 --> 1185/1270Amulet_Heart\n",
      "2023-12-17 12:13:24 --> 1196/1270-Sazamia-\n",
      "2023-12-17 12:15:45 --> 1206/1270-Abhishek-\n",
      "2023-12-17 12:15:50 --> 1207/1270-Based-\n",
      "2023-12-17 12:16:01 --> 1209/1270Barusu011\n",
      "2023-12-17 12:16:12 --> 1211/1270Adnash\n",
      "2023-12-17 12:17:22 --> 1215/1270Amaryllidaceae\n",
      "2023-12-17 12:20:07 --> 1225/1270Allen2020\n",
      "2023-12-17 12:20:41 --> 1229/1270A1B2C3D4E5F6G7H8\n",
      "2023-12-17 12:24:57 --> 1252/12705615160554\n",
      "2023-12-17 12:28:58 --> 1270/1270"
     ]
    }
   ],
   "source": [
    "#%%notify -m \"animelist finish\"\n",
    "\n",
    "USER_PATH = os.path.join(BASE_PATH, \"users\")  # Specify the user directory\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(USER_PATH, exist_ok=True)\n",
    "\n",
    "for i, (user_id, username) in enumerate(users):\n",
    "    if user_id <= last_revised_users:\n",
    "        continue\n",
    "\n",
    "    now = datetime.now()\n",
    "    print(f'\\r{str(now).split(\".\")[0]} --> {i+1}/{len(users)}', end=\"\")\n",
    "    page = 1\n",
    "    all_animes = []\n",
    "    offset = 0\n",
    "    while True:\n",
    "    # link = f\"https://api.jikan.moe/v4/user/{username}/animelist/all?page={page}\"\n",
    "        link = f\"https://myanimelist.net/animelist/{username}/load.json?offset={offset}\"\n",
    "        try:\n",
    "            time.sleep(4.2)\n",
    "            data = requests.get(link, timeout=15)\n",
    "        except KeyboardInterrupt:\n",
    "            raise KeyboardInterrupt()\n",
    "        except:  # Other exception wait 2 min and try again\n",
    "            time.sleep(120)\n",
    "            continue\n",
    "\n",
    "\n",
    "        # if data.status_code != 200:\n",
    "        #     break\n",
    "\n",
    "        data = json.loads(data.text)\n",
    "\n",
    "        for anime in data:\n",
    "            try:\n",
    "                all_animes.append((anime[\"anime_id\"], anime[\"score\"], anime[\"status\"], anime[\"num_watched_episodes\"]))\n",
    "            except:\n",
    "                print(username)\n",
    "\n",
    "        offset += 300\n",
    "        if len(data) < 300:\n",
    "            break\n",
    "\n",
    "    if len(all_animes) != 0:\n",
    "        \n",
    "        user_file_path = os.path.join(USER_PATH, f\"{user_id}.csv\")\n",
    "        with open(user_file_path, \"w\") as f1:\n",
    "            f1.write(f\"anime_id,score,watching_status,watched_episodes\\n\")\n",
    "            for anime_id, anime_score, watching_status, watched_episodes in all_animes:\n",
    "                f1.write(\n",
    "                    f\"{anime_id},{anime_score},{watching_status},{watched_episodes}\\n\"\n",
    "                )\n",
    "\n",
    "    revised_users = user_id\n",
    "    with open(f\"{BASE_PATH}/_last_revised_users.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
    "        file.write(f\"{user_id}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Download Anime HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1144/1144         \n",
      "18217\n"
     ]
    }
   ],
   "source": [
    "unique_anime = set()\n",
    "folder = os.listdir(USER_PATH)\n",
    "for i, user_file in enumerate(folder):\n",
    "    if \".csv\" not in user_file:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\r{i + 1}/{len(folder)}\", end=\"\")\n",
    "    with open(f\"{USER_PATH}/{user_file}\", \"r\") as file:\n",
    "        file.readline()\n",
    "        for line in file:\n",
    "            anime = line.strip().split(\",\")[0]\n",
    "            unique_anime.add(anime)\n",
    "\n",
    "print(\"         \")\n",
    "print(len(unique_anime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX = 7  # MAX SECOND TO WAIT PER REQUEST\n",
    "MIN = 4  # MIN SECONDS TO WAIT PER REQUEST\n",
    "\n",
    "\n",
    "def sleep():\n",
    "    time_to_sleep = random.random() * (MAX - MIN) + MIN\n",
    "    time.sleep(time_to_sleep)\n",
    "\n",
    "\n",
    "def get_link_by_text(soup, anime_id, text):\n",
    "    links = list(filter(lambda x: anime_id in x[\"href\"], soup.find_all(\"a\", text=text)))\n",
    "    return links[0][\"href\"]\n",
    "\n",
    "\n",
    "def save(path, data):\n",
    "    with open(path, \"w\", encoding=\"UTF-8\") as file:\n",
    "        file.write(data)\n",
    "\n",
    "\n",
    "def save_link(link, anime_id, name):\n",
    "    sleep()\n",
    "    path = f\"{HTML_PATH}/{anime_id}/{name}.html\"\n",
    "    data = requests.get(link)\n",
    "    soup = BeautifulSoup(data.text, \"html.parser\")\n",
    "    soup.script.decompose()\n",
    "    save(path, soup.prettify())\n",
    "    return soup\n",
    "\n",
    "\n",
    "def save_reviews(link, anime_id):\n",
    "    page = 1\n",
    "    while True:\n",
    "        sleep()\n",
    "        actual_link = f\"{link}?p={page}\"\n",
    "        data = requests.get(actual_link)\n",
    "        soup = BeautifulSoup(data.text, \"html.parser\")\n",
    "        reviews = soup.find_all(\"a\", text=\"Overall Rating\")\n",
    "        if len(reviews) == 0:\n",
    "            break\n",
    "\n",
    "        path = f\"{HTML_PATH}/{anime_id}/reviews_{page}.html\"\n",
    "        soup.script.decompose()\n",
    "        save(path, soup.prettify())\n",
    "        page += 1\n",
    "\n",
    "\n",
    "def scrap_anime(anime_id):\n",
    "    path = f\"{HTML_PATH}/{anime_id}\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    sleep()\n",
    "    data = requests.get(f\"https://myanimelist.net/anime/{anime_id}\")\n",
    "\n",
    "    anime_info = data.text\n",
    "    soup = BeautifulSoup(anime_info, \"html.parser\")\n",
    "    soup.script.decompose()\n",
    "    save(f\"{HTML_PATH}/{anime_id}/details.html\", soup.prettify())\n",
    "\n",
    "    link_review = get_link_by_text(soup, anime_id, \"Reviews\")\n",
    "    link_recomendations = get_link_by_text(soup, anime_id, \"Recommendations\")\n",
    "    link_stats = get_link_by_text(soup, anime_id, \"Stats\")\n",
    "    link_staff = get_link_by_text(soup, anime_id, \"Characters & Staff\")\n",
    "    link_pictures = get_link_by_text(soup, anime_id, \"Pictures\")\n",
    "\n",
    "    save_link(link_pictures, anime_id, \"pictures\")\n",
    "    save_link(link_staff, anime_id, \"staff\")\n",
    "    save_link(link_stats, anime_id, \"stats\")\n",
    "    save_link(link_recomendations, anime_id, \"recomendations\")\n",
    "    save_reviews(link_review, anime_id)\n",
    "\n",
    "\n",
    "def zipdir(path, ziph):\n",
    "    # ziph is zipfile handle\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(\n",
    "                os.path.join(root, file),\n",
    "                os.path.relpath(os.path.join(root, file), path),\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/18217"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:37: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292/18217"
     ]
    }
   ],
   "source": [
    "#%%notify -m \"Anime scrapping finish\"\n",
    "\n",
    "for i, anime_id in enumerate(unique_anime):\n",
    "    if os.path.isfile(f\"{HTML_PATH}/{anime_id}.zip\"):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\r{i+1}/{len(unique_anime)}\", end=\"\")\n",
    "\n",
    "    try:\n",
    "        scrap_anime(anime_id)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except:  # Other exception wait 2 min and try again\n",
    "        time.sleep(120)\n",
    "        continue\n",
    "\n",
    "    path = f\"{HTML_PATH}/{anime_id}\"\n",
    "    zipf = zipfile.ZipFile(f\"{path}.zip\", \"w\", zipfile.ZIP_DEFLATED)\n",
    "    zipdir(path, zipf)\n",
    "    zipf.close()\n",
    "\n",
    "    shutil.rmtree(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawl",
   "language": "python",
   "name": "crawl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
